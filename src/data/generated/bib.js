define({ entries : {
    "BhattacharyaRanjeeta2024Stmh": {
        "abstract": "In the world of enterprise-level applications, the construction and utilisation of large language models (LLMs) carry a paramount significance, accompanied by the crucial task of mitigating hallucinations. These instances of generating factually inaccurate information pose challenges during both the initial development phase of LLMs and the subsequent refinement process through prompt engineering. This paper delves into a variety of approaches such as retrieval augmented generation, advanced prompting methodologies, harnessing the power of knowledge graphs, construction of entirely new LLMs from scratch etc, aimed at alleviating these challenges. The paper also underscores the indispensable role of human oversight and user education in addressing this evolving issue. As the field continues to evolve, the importance of continuous vigilance and adaptation cannot be overstated, with a focus on refining strategies to effectively combat hallucinations within LLMs.",
        "author": "Bhattacharya, Ranjeeta",
        "doi": "10.69554/NXXB8234",
        "journal": "Applied Marketing Analytics: The Peer-Reviewed Journal",
        "keywords": "type:journal article, Large Language Model, Hallucination, Llm, Prompt Engineering, Rag",
        "number": "1",
        "publisher": "Henry Stewart Publications",
        "series": "AMA",
        "title": "Strategies to mitigate hallucinations in large language models",
        "type": "article",
        "url": "https://doi.org/10.69554/NXXB8234",
        "volume": "10",
        "year": "2024"
    },
    "CuiYachao2025KKgL": {
        "abstract": "Recommendation algorithms typically leverage auxiliary information such as user reviews and knowledge graphs to enhance algorithm performance, thereby alleviating data sparsity and cold start issues. Recently, researchers have increasingly employed large language models, which boast powerful natural language understanding capabilities, to further improve recommendation systems. However, these models often suffer from hallucination problems. Moreover, integrating heterogeneous information, such as reviews and knowledge graphs, can introduce new noise, potentially impairing recommendation performance. Knowledge graphs, as tightly organized structured knowledge bases, can assist in addressing the hallucination problem and heterogeneous information fusion problem of LLMs. To effectively address the aforementioned issues, we propose the Knowledge Graph-Enhanced Large Language Model Sentiment Extraction for the Personalized Recommendation Model (KLLMs4Rec). It aims to solve the LLMs hallucination problem and the noise problem caused by the fusion of heterogeneous information in recommender systems, and provide users with more accurate, diverse and novel personalized recommendations. To address the hallucination problem when extracting user sentiments from reviews with LLMs, we designed a knowledge graph-enhanced prompt template. It is worth noting that this scheme also solves the noise issue of heterogeneous information fusion. Additionally, to further expand user preferences extracted from reviews, this paper proposes a new hierarchical sentiment attention graph convolutional network, which utilizes three sentiment weight schemes to propagate user personalized preferences on the knowledge graph. Extensive experiments on the Movielens-20 m, Amazon-book, and Yelp datasets demonstrate that our model surpasses current leading methods while effectively addressing the hallucination problem of LLMs and the noise problem of heterogeneous information fusion. Constructing Knowledge Graph-enhanced LLMs prompt templates.\u2022Extracted two types of aspectual emotions using LLMs.\u2022Designed a hierarchical sentiment-aware graph convolutional network.\u2022Solved the hallucination and the heterogeneous information fusion problem of LLMs.",
        "author": "Cui, Yachao and Wang, Kaiguang and Yu, Hongli and Guo, Xiaoxu and Cao, Han",
        "doi": "10.1016/j.eswa.2025.127430",
        "journal": "Expert systems with applications",
        "keywords": "type:journal article, Personalized recommendation, Knowledge graphs, Large Language Models, Text sentiment analysis",
        "publisher": "Elsevier Ltd",
        "series": "ESWA",
        "title": "KLLMs4Rec: Knowledge graph-enhanced LLMs sentiment extraction for personalized recommendations",
        "type": "article",
        "url": "https://doi.org/10.1016/j.eswa.2025.127430",
        "volume": "282",
        "year": "2025"
    },
    "FarquharSebastian2024Dhil": {
        "abstract": "Large language model (LLM) systems, such as ChatGPT or Gemini, can show impressive reasoning and question-answering capabilities but often 'hallucinate' false outputs and unsubstantiated answers. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents or untrue facts in news articles and even posing a risk to human life in medical domains such as radiology. Encouraging truthfulness through supervision or reinforcement has been only partially successful. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations\u2014confabulations\u2014which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability",
        "author": "Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin",
        "doi": "10.1038/s41586-024-07421-0",
        "journal": "Nature (London)",
        "keywords": "type:journal article, Large Language Models, Hallucinations, Confabulations, Semantic Entropy, Uncertainty estimation, Natural Language Processing, AI Safety, Question Answering, Factuality Detection",
        "number": "8017",
        "publisher": "Nature Publishing Group UK",
        "series": "Nature",
        "title": "Detecting hallucinations in large language models using semantic entropy",
        "type": "article",
        "url": "https://www.nature.com/articles/s41586-024-07421-0",
        "volume": "630",
        "year": "2024"
    },
    "HuangXiang2025Casv": {
        "abstract": "The rise of Large Language Models (LLMs) has further led to the development of text summarization techniques and also brought more attention to the problem of hallucination in the research of text summarization. Existing work in current text summarization research based on LLMs typically uses In-Context Learning (ICL) to supply accurate (document-summary) pairs of samples to the model, thus allowing the model to be more explicit in predicting the target. However, in this way, models can only determine what to do, without explicitly prohibiting what models cannot do. It is highly likely to lead to increased hallucinations due to excessive model-free play. In this paper, to alleviate the problem of hallucination in text summarization based on LLMs, we propose CL2Sum, a method that combines Contrastive Learning (CL) and ICL for summarization. After analysing the generated summaries of LLMs and summarising their hallucination types, we provided the models with accurate summaries and summaries containing hallucinations as ICL instances, either automatically or artificially. It aims to guide the model to make accurate predictions according to positive samples while also avoiding hallucinations similar to those in negative samples. Finally, a series of comparative experiments were conducted on summary datasets of different lengths and languages. The results show that CL2Sum effectively alleviates the hallucination problem of text summaries while also improving the overall quality of the generated summaries. Moreover, it can be widely adapted to text summarization tasks in different scenarios with a certain degree of robustness.",
        "author": "Huang, Xiang and Nong, Qiong and Wang, Xiaobo and Zhang, Hongcheng and Du, Kunpeng and Yin, Chunlin and Yang, Li and Yan, Bin and Zhang, Xuan",
        "doi": "10.1007/s40747-025-01795-y",
        "journal": "Complex & intelligent systems",
        "keywords": "type:journal article, Abstractive summarization, Hallucinations, Complexity, Computational Intelligence, Contrastive learning, Data Structures and Information Theory, Engineering, Hallucination, In-context learning, Large language models, LLMs, Original Article, Summaries",
        "number": "3",
        "publisher": "Springer International Publishing",
        "series": "CIS",
        "title": "Cl2sum: abstractive summarization via contrastive prompt constructed by LLMs hallucination",
        "type": "article",
        "url": "https://doi.org/10.1007/s40747-025-01795-y",
        "volume": "11",
        "year": "2025"
    },
    "LiuYanyi2025Rhol": {
        "abstract": "With the widespread application of large language models (LLMs) in natural language processing (NLP), hallucinations have become a significant impediment to their effective use of LLMs in industry applications. To address this challenge, we integrate existing hallucination detection and mitigation methods into a unified hallucination detection and mitigation framework. The framework consists of four main components: output parser, reference parser, fact verifier, and mitigator. These components collectively consolidate various hallucination detection and mitigation methods. Within this unified framework, we introduce the hierarchical semantic piece (HSP) for hallucination detection and mitigation. The HSP method extracts multi-granularity semantic pieces from both the reference material and the generated text. Sentence-level semantic pieces encapsulate global semantic information, while entity-level semantic pieces handle local semantic information. This method verifies the consistency between the generated text and the reference text at corresponding granularities, thereby enhancing the effectiveness of hallucination detection and mitigation. Experimental results show that the HSP method is very effective in detecting and mitigating hallucinations and shows lower computational resource consumption. Our method has great potential and promises for industry applications that rely on professionalism and reliability.",
        "author": "Liu, Yanyi and Yang, Qingwen and Tang, Jiawei and Guo, Tiezheng and Wang, Chen and Li, Pan and Xu, Sai and Gao, Xianlin and Li, Zhi and Liu, Jun and Wen, Yingyou",
        "doi": "10.1007/s40747-025-01833-9",
        "journal": "Complex & intelligent systems",
        "keywords": "type:research article, Large language models, Complexity, Computational Intelligence, Data Structures and Information Theory, Effectiveness, Engineering, Hallucination detection, Hallucination mitigation, Hallucinations, Industrial applications, Industry applications, Natural language processing, Original Article, Parsers, Reference materials, Semantics",
        "number": "5",
        "publisher": "Springer International Publishing",
        "series": "CIS",
        "title": "Reducing hallucinations of large language models via hierarchical semantic piece",
        "type": "article",
        "url": "https://doi.org/10.1007/s40747-025-01833-9",
        "volume": "11",
        "year": "2025"
    },
    "R10.11453703155": {
        "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.",
        "author": "Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting",
        "doi": "10.1145/3703155",
        "journal": "ACM Trans. Inf. Syst.",
        "keywords": "type:survey article, Large Language Models, Hallucination, Factuality, Faithfulness",
        "number": "2",
        "publisher": "Association for Computing Machinery",
        "series": "TIST",
        "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
        "type": "article",
        "url": "https://doi.org/10.1145/3703155",
        "volume": "43",
        "year": "2025"
    },
    "VerspoorKarin2024'fwf": {
        "abstract": "The number of errors produced by an LLM can be reduced by grouping its outputs into semantically similar clusters. Remarkably, this task can be performed by a second LLM, and the method's efficacy can be evaluated by a third. Confabulations detected by clustering semantically similar LLM responses.",
        "author": "Verspoor, Karin",
        "doi": "10.1038/d41586-024-01641-0",
        "journal": "Nature (London)",
        "keywords": "type:journal article, Large Language Models, Hallucinations, Confabulations, Semantic entropy, Uncertainty detection, Model evaluation, Semantic equivalence",
        "number": "8017",
        "publisher": "Nature Publishing Group UK",
        "series": "Nature",
        "title": "'Fighting fire with fire' \u2014 using LLMs to combat LLM hallucinations",
        "type": "article",
        "url": "https://www.nature.com/articles/d41586-024-01641-0",
        "volume": "630",
        "year": "2024"
    },
    "XuKehan2025CARG": {
        "abstract": "The Retrieval-Augmented Generation (RAG) framework enhances Large Language Models (LLMs) by retrieving relevant knowledge to broaden their knowledge boundaries and mitigate factual hallucinations stemming from knowledge gaps. However, the RAG Framework faces challenges in effective knowledge retrieval and utilization; invalid or misused knowledge will interfere with LLM generation, reducing reasoning efficiency and answer quality. Existing RAG methods address these issues by decomposing and expanding queries, introducing special knowledge structures, and using reasoning process evaluation and feedback. However, the linear reasoning structures limit complex thought transformations and reasoning based on intricate queries. Additionally, knowledge retrieval and utilization are decoupled from reasoning and answer generation, hindering effective knowledge support during answer generation. To address these limitations, we propose the CRP-RAG framework, which employs reasoning graphs to model complex query reasoning processes more comprehensively and accurately. CRP-RAG guides knowledge retrieval, aggregation, and evaluation through reasoning graphs, dynamically adjusting the reasoning path based on evaluation results and selecting knowledge-sufficiency paths for answer generation. CRP-RAG outperforms the best LLM and RAG baselines by 2.46 in open-domain QA, 7.43 in multi-hop reasoning, and 4.2 in factual verification. Experiments also show the superior factual consistency and robustness of CRP-RAG over existing RAG methods. Extensive analyses confirm its accurate and fact-faithful reasoning and answer generation for complex queries.",
        "author": "Xu, Kehan and Zhang, Kun and Li, Jingyuan and Huang, Wei and Wang, Yuanzhuo",
        "doi": "10.3390/electronics14010047",
        "journal": "Electronics (Basel)",
        "keywords": "type:journal article, Cage Nicolas, Cognition & reasoning, Graphs, Hallucinations, Knowledge representation, Large language models, Leoni T\u00e9a, Planning, Queries, Reasoning, Retrieval",
        "number": "1",
        "publisher": "MDPI AG",
        "series": "Electronics",
        "title": "CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning",
        "type": "article",
        "url": "https://doi.org/10.3390/electronics14010047",
        "volume": "14",
        "year": "2025"
    },
    "YangYi2025MHaC": {
        "abstract": "The emergence of large language models (LLMs), such as GPT and Claude, has revolutionized AI by enabling general and domain-specific natural language tasks. However, hallucinations, characterized by false or inaccurate responses, pose serious limitations, particularly in critical fields like medicine and law, where any compromise in reliability can lead to severe consequences. This paper addresses the hallucination issue by proposing a multi-agent LLM framework, incorporating adversarial and voting mechanisms. Specifically, the framework employs repetitive inquiries and error logs to mitigate hallucinations within single LLMs, while adversarial debates and voting mechanisms enable cross-verification among multiple agents, thereby determining when external knowledge retrieval is necessary. Additionally, an entropy compression technique is introduced to enhance communication efficiency by reducing token usage and task completion time. Experimental results demonstrate that the framework significantly improves accuracy, showing a steady increase in composite accuracy across 20 evaluation batches while reducing hallucinations and optimizing task completion time. Notably, the dynamic weighting mechanism effectively prioritized high-performing models, leading to a reduction in error rates and improved consistency in the final responses.",
        "author": "Yang, Yi and Ma, Yitong and Feng, Hao and Cheng, Yiming and Han, Zhu",
        "doi": "10.3390/app15073676",
        "journal": "Applied sciences",
        "keywords": "type:journal article, Debates, voting mechanism, Voting, Accuracy, adversarial debate, Analysis, Chatbots, Communication, Data collection, Efficiency, Entropy, external knowledge retrieval, Generative artificial intelligence, hallucination reduction, Hallucinations, Hallucinations and illusions, Language, Large language models, multi-agents, Performance evaluation",
        "number": "7",
        "publisher": "MDPI AG",
        "series": "AppSci",
        "title": "Minimizing Hallucinations and Communication Costs: Adversarial Debate and Voting Mechanisms in LLM-Based Multi-Agents",
        "type": "article",
        "url": "https://doi.org/10.3390/app15073676",
        "volume": "15",
        "year": "2025"
    },
    "ZhangWan2025HMfR": {
        "abstract": "Retrieval-augmented generation (RAG) leverages the strengths of information retrieval and generative models to enhance the handling of real-time and domain-specific knowledge. Despite its advantages, limitations within RAG components may cause hallucinations, or more precisely termed confabulations in generated outputs, driving extensive research to address these limitations and mitigate hallucinations. This review focuses on hallucination in retrieval-augmented large language models (LLMs). We first examine the causes of hallucinations from different sub-tasks in the retrieval and generation phases. Then, we provide a comprehensive overview of corresponding hallucination mitigation techniques, offering a targeted and complete framework for addressing hallucinations in retrieval-augmented LLMs. We also investigate methods to reduce the impact of hallucination through detection and correction. Finally, we discuss promising future research directions for mitigating hallucinations in retrieval-augmented LLMs.",
        "author": "Zhang, Wan and Zhang, Jing",
        "doi": "10.3390/math13050856",
        "journal": "Mathematics (Basel)",
        "keywords": "type:survey article, Large language models, Artificial intelligence, Classification, Confabulation, Engineering, hallucination, hallucination mitigation, Hallucinations, Information retrieval, Knowledge, Language, Natural language processing, Real time, retrieval-augmented generation",
        "number": "5",
        "publisher": "MDPI AG",
        "series": "Mathematics",
        "title": "Hallucination Mitigation for Retrieval-Augmented Large Language Models: A Review",
        "type": "article",
        "url": "https://doi.org/10.3390/math13050856",
        "volume": "13",
        "year": "2025"
    }
}});